{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation\n",
    "This notebook showcases the building blocks that can be used for building a simple data curation pipeline using [NeMo Curator](https://github.com/NVIDIA/NeMo-Curator).\n",
    "\n",
    "## Reading Materials\n",
    "Before proceeding, we highly recommend looking through the following deep dive blog posts that walk you through building data curation pipelines using NeMo Curator:\n",
    "- [Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator](https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator/)\n",
    "- [Curating Custom Datasets for LLM Parameter-Efficient Fine-Tuning with NVIDIA NeMo Curator](https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-parameter-efficient-fine-tuning-with-nvidia-nemo-curator/)\n",
    "\n",
    "In this notebook, we will use the [Law-StackExchange dataset](https://huggingface.co/datasets/ymoslem/Law-StackExchange) for this pipeline, which is a dataset of legal question/answers scraped from the Stack Exchange website.\n",
    "\n",
    "## Setup and Requirements\n",
    "Before proceeding, you need to install one dependency to follow along. Execute the following cell before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /home/mmaghoumi/.local/lib/python3.10/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /home/mmaghoumi/.local/lib/python3.10/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /home/mmaghoumi/.local/lib/python3.10/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's setup some environment variables, as well as path variables that will be used for storing the curated data, as well as intermediate temporary files that are required for this notebooks to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASK_DATAFRAME__QUERY_PLANNING\"] = \"False\"  # Needed for running Curator on the GPU\n",
    "\n",
    "NOTEBOOK_DIR = os.path.abspath(\"\")\n",
    "DATA_DIR = os.path.join(NOTEBOOK_DIR, \"data\")\n",
    "TEMP_DIR = os.path.join(NOTEBOOK_DIR, \".temp\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import everything we need for building our data curation pipeline. For your conveniene, we've provided the document builder implementations which allow you to download the dataset from HuggingFace and convert it into a data frame format.\n",
    "\n",
    "We have additionally implemented a score-based filterer that allows you to filter the dataset rows using the score values assigned to each question. You can use this implementation as the basis for creating your own filtering/scoring mechanisms using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import WordCountFilter\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.modules.config import SemDedupConfig\n",
    "from nemo_curator.modules.semantic_dedup import SemDedup\n",
    "from nemo_curator.utils.file_utils import expand_outdir_and_mkdir\n",
    "from nemo_curator import ScoreFilter, Sequential\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "# Importing helper functions\n",
    "from helpers.filters import FilterLowScores\n",
    "from helpers.docbuilder import download_and_convert_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's decide the compute resources we'd like to use for running our data curation pipeline. NeMo Curator uses Dask to orchestrate scalable data processing. As such, it needs to know what resources to use. \n",
    "\n",
    "For the purposes of this exercise, we will use the GPU and instruct NeMo Curator to use 8 CPU workers. While most NeMo Curator functionalities can be executed on the CPU, some modules (such as semantic deduplication) can only be executed on the GPU.\n",
    "\n",
    "Note that you can increase or decrease the number of CPU workers depending on the runtime environment. Keep in mind that each CPU worker gets allocated a fixed amount of the total available system memory (RAM). Thus, if the environment does not have enough memory available, Dask opeartions might fail.\n",
    "\n",
    "Once we have decided on the resources to use, we can initialize our Dask cluster and start using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"  # It can be either \"cpu\" or \"gpu\"\n",
    "n_workers = 8  # Number of workers to use for Dask. If running out of memory, try reducing this.\n",
    "client = get_client(device, n_workers=n_workers, set_torch_to_use_rmm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Main Data Curation Pipeline\n",
    "\n",
    "We start by downloading and converting the dataset into a suitable format. This is done via the document builders that we have provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory:  /home/mmaghoumi/git/dl-tme/2024/odsc-hackathon-october-2024/all-in-one/data/raw\n",
      "File '/home/mmaghoumi/git/dl-tme/2024/odsc-hackathon-october-2024/all-in-one/data/raw/law-stackexchange-questions-answers.json' already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "dataset_df = download_and_convert_dataset(DATA_DIR)\n",
    "raw_dataset = DocumentDataset.from_pandas(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our data curation pipeline. The pipeline we define here is very simple, as it contains basic filtering operations, as well as GPU-based semantic deduplication.\n",
    "Note that in order to use the modules that need a GPU, the dataset has to be converted to the `cudf` backend.\n",
    "\n",
    "We point the semantic deduplication module to a config file that defines the exact model and parameters to use for performing semantic deduplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_dedupe(dataset: DocumentDataset):\n",
    "    \"\"\"\n",
    "    Perform semantic deduplication on the given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: The input DocumentDataset.\n",
    "\n",
    "    Returns:\n",
    "        The deduplicated DocumentDataset.\n",
    "    \"\"\"\n",
    "    # Clean up the temporary directory to ensure everything is clean.\n",
    "    if os.path.isdir(TEMP_DIR):\n",
    "        os.system(f\"rm -rf {TEMP_DIR}\")\n",
    "\n",
    "    semdedup_config = SemDedupConfig.from_yaml(\"helpers/sem_dedup_config.yaml\")\n",
    "    expand_outdir_and_mkdir(semdedup_config.cache_dir)\n",
    "    semdup = SemDedup(semdedup_config)\n",
    "    dedup_ids = semdup(dataset)\n",
    "    # When there are few duplicates we can compute the results to a list and use `isin`.\n",
    "    result = dataset.df[dataset.df[\"id\"].isin(dedup_ids.df[\"id\"].compute())]\n",
    "    return DocumentDataset(result)\n",
    "\n",
    "def run_curation_pipeline(dataset: DocumentDataset, device: str) -> DocumentDataset:\n",
    "    print(f\"Running curation pipeline on '{device}'...\")\n",
    "    orig_dataset = dataset\n",
    "\n",
    "    cpu_curation_steps = Sequential(\n",
    "        [\n",
    "            #\n",
    "            # Modifications\n",
    "            #\n",
    "            # Unify the text encoding to Unicode.\n",
    "            Modify(UnicodeReformatter(), text_field=\"title\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"question\"),\n",
    "            #\n",
    "            # Filtering\n",
    "            #\n",
    "            # Filter out records based on the question word counts.\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=50, max_words=500),\n",
    "                text_field=\"question\",\n",
    "                score_type=int,\n",
    "            ),\n",
    "            # Filter out records where the question has a negative score.\n",
    "            ScoreFilter(\n",
    "                FilterLowScores(score_threshold=0),\n",
    "                text_field=\"question_score\",\n",
    "                score_type=bool,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Run the CPU curation steps.\n",
    "    dataset = cpu_curation_steps(dataset)\n",
    "\n",
    "    # Perform semantic deduplication on the dataset (if the device is GPU).\n",
    "    if device == \"gpu\":\n",
    "        # Create a \"text\" field comprised of the title and the question.\n",
    "        # Note that the \"text\" field here must be the same as the setting specified for `input_column` in sem_dedup_config.yaml file.\n",
    "        # The algorithm looks at this field to find semantically similar records.\n",
    "        dataset.df[\"text\"] = (\n",
    "            dataset.df[\"title\"]\n",
    "            + \"\\n\"\n",
    "            + dataset.df[\"question\"]\n",
    "        )\n",
    "        # Convert the dataset to a GPU backend.\n",
    "        dataset.df = dataset.df.to_backend(\"cudf\")\n",
    "        dataset = semantic_dedupe(dataset)\n",
    "        # Delete the text field as it is no longer needed.\n",
    "        del dataset.df[\"text\"]\n",
    "        # Convert the dataset back to a CPU backend.\n",
    "        dataset.df = dataset.df.to_backend(\"pandas\")\n",
    "\n",
    "    dataset = dataset.persist()\n",
    "    # Drop the columns that are no longer needed.\n",
    "    dataset.df = dataset.df.drop(columns=[\"answer\", \"answer_score\", \"question_score\"])\n",
    "    orig_len = len(orig_dataset.df)\n",
    "    new_len = len(dataset.df)\n",
    "\n",
    "    print(f\"Original dataset length: {orig_len}\")\n",
    "    print(f\"New dataset length: {new_len}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the pipeline and get our final dataset. This will take some time to execute, especially if semantic deduplication is enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running curation pipeline on 'cpu'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 59.17 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 59.17 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset length: 24343\n",
      "New dataset length: 18759\n"
     ]
    }
   ],
   "source": [
    "curated_dataset = run_curation_pipeline(raw_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify the final columns that we would like our dataset to have. Depending on how you plan on consuming this dataset for training, you may decide to introduce other arbitrary columns to help the model learn better.\n",
    "\n",
    "Also, this is a great place to add system or instruction prompts to every record, in case you intend to use the same instruction prompt for every record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset columns: Index(['filename', 'id', 'input', 'output'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\\n\\n\"\n",
    "\n",
    "df = curated_dataset.df\n",
    "df[\"input\"] = SYSTEM_PROMPT + \"TITLE:\\n\" + df[\"title\"] + \"\\n\\n\" + \"QUESTION:\\n\" + df[\"question\"]\n",
    "df[\"output\"] = df[\"tags\"]\n",
    "df[\"filename\"] = \"law-stackexchange-curated.jsonl\"\n",
    "\n",
    "df = df.drop(columns=[\"title\", \"question\", \"tags\"])\n",
    "curated_dataset.df = df\n",
    "print(f\"Dataset columns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the final dataset is ready, we can write it into a JSONL file and start using it for model training.\n",
    "\n",
    "> NOTE: The curated dataset will be written under `curator/data/curated_dataset/law-stackexchange-curated.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated dataset columns: Index(['filename', 'id', 'input', 'output'], dtype='object')\n",
      "\n",
      "Saving curated dataset to '/home/mmaghoumi/git/dl-tme/2024/odsc-hackathon-october-2024/all-in-one/data/curated_dataset'...\n",
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "print(f\"Curated dataset columns: {curated_dataset.df.columns}\")\n",
    "result_fp = os.path.join(DATA_DIR, \"curated_dataset\")\n",
    "print()\n",
    "print(f\"Saving curated dataset to '{result_fp}'...\")\n",
    "curated_dataset.to_json(result_fp, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Spliting the Dataset\n",
    "\n",
    "Before starting the model training procedure, let's split the dataset we've just curated into `training`, `validation` and `test` splits with 80/10/10 ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 18759\n",
      "After splitting:\n",
      "    Train size: 15009\n",
      "    Validation size: 1875\n",
      "    Test size: 1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 17.38 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partitions\n",
      "Writing to disk complete for 1 partitions\n",
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "df = curated_dataset.df.compute()\n",
    "\n",
    "# Some sanity checks\n",
    "assert len(df) > 0, \"The dataset is empty.\"\n",
    "assert VAL_RATIO >= 0 and VAL_RATIO <= 1, \"VAL_RATIO must be between 0 and 1.\"\n",
    "assert TEST_RATIO >= 0 and TEST_RATIO <= 1, \"TEST_RATIO must be between 0 and 1.\"\n",
    "assert VAL_RATIO + TEST_RATIO < 1, \"VAL_RATIO + TEST_RATIO must be less than 1.\"\n",
    "val_size = int(len(df) * VAL_RATIO)\n",
    "test_size = int(len(df) * TEST_RATIO)\n",
    "output_dir = f\"{DATA_DIR}/curated_dataset/split\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split the data into training and temporary sets\n",
    "train_df, temp_df = train_test_split(df, test_size=val_size + test_size, random_state=42)\n",
    "# Split the temporary set into validation and test sets\n",
    "val_df, test_df = train_test_split(temp_df, test_size=test_size, random_state=42)\n",
    "\n",
    "print(f\"Original size: {len(df)}\")\n",
    "print(\"After splitting:\")\n",
    "print(f\"    Train size: {len(train_df)}\")\n",
    "print(f\"    Validation size: {len(val_df)}\")\n",
    "print(f\"    Test size: {len(test_df)}\")\n",
    "\n",
    "train_df[\"filename\"] = \"train.jsonl\"\n",
    "val_df[\"filename\"] = \"val.jsonl\"\n",
    "test_df[\"filename\"] = \"test.jsonl\"\n",
    "\n",
    "DocumentDataset.from_pandas(train_df).to_json(output_dir, write_to_filename=True)\n",
    "DocumentDataset.from_pandas(val_df).to_json(output_dir, write_to_filename=True)\n",
    "DocumentDataset.from_pandas(test_df).to_json(output_dir, write_to_filename=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
