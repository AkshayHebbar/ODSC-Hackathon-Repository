{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation\n",
    "This notebook showcases the building blocks that can be used for building a simple data curation pipeline using [NeMo Curator](https://github.com/NVIDIA/NeMo-Curator).\n",
    "\n",
    "## Reading Materials\n",
    "Before proceeding, we highly recommend looking through the following deep dive blog posts that walk you through building data curation pipelines using NeMo Curator:\n",
    "- [Curating Custom Datasets for LLM Training with NVIDIA NeMo Curator](https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-training-with-nvidia-nemo-curator/)\n",
    "- [Curating Custom Datasets for LLM Parameter-Efficient Fine-Tuning with NVIDIA NeMo Curator](https://developer.nvidia.com/blog/curating-custom-datasets-for-llm-parameter-efficient-fine-tuning-with-nvidia-nemo-curator/)\n",
    "\n",
    "Also, please checkout [our tutorials](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials) in the repository to learn more about various functionalities that NeMo Curator provides.\n",
    "\n",
    "In this notebook, we will use the [Law-StackExchange dataset](https://huggingface.co/datasets/ymoslem/Law-StackExchange) for this pipeline, which is a dataset of legal question/answers scraped from the Stack Exchange website. This notebook is the summarized version of our existing [synthetic data generation tutorial](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/peft-curation-with-sdg). Feel free to go through that tutorial to gain a better understanding of various NeMo Curator facilities.\n",
    "\n",
    "## Setup and Requirements\n",
    "The NeMo dependencies are already installed in the container. However, before proceeding you need to install one dependency to follow along. Execute the following cell before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.5 jupyterlab-widgets-3.0.13 widgetsnbextension-4.0.13\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, let's setup some environment variables, as well as path variables that will be used for storing the curated data, as well as intermediate temporary files that are required for this notebooks to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"DASK_DATAFRAME__QUERY_PLANNING\"] = \"False\"  # Needed for running Curator on the GPU\n",
    "\n",
    "NOTEBOOK_DIR = os.path.abspath(\"\")\n",
    "DATA_DIR = os.path.join(NOTEBOOK_DIR, \"data\")\n",
    "TEMP_DIR = os.path.join(NOTEBOOK_DIR, \".temp\")\n",
    "os.makedirs(DATA_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now import everything we need to build our data curation pipeline. For your conveniene, we've provided the document builder implementations that allow you to download the dataset from HuggingFace and convert it into a Pandas `DataFrame`.\n",
    "\n",
    "We have additionally implemented a score-based filter that allows you to filter the dataset rows using the score values assigned to each question. You can use this implementation as the basis for creating your own filtering/scoring mechanisms using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.filters import WordCountFilter, SymbolsToWordsFilter, RepeatingTopNGramsFilter\n",
    "from nemo_curator.modifiers import UnicodeReformatter\n",
    "from nemo_curator.modifiers import DocumentModifier\n",
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "from nemo_curator.utils.file_utils import expand_outdir_and_mkdir\n",
    "from nemo_curator import ScoreFilter, Sequential\n",
    "from nemo_curator.modules.modify import Modify\n",
    "\n",
    "# Importing helper functions\n",
    "from helpers.filters import FilterLowScores\n",
    "from helpers.docbuilder import download_and_convert_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, let's decide the compute resources we'd like to use for running our data curation pipeline. NeMo Curator uses Dask to orchestrate scalable data processing. As such, it needs to know what resources to use. \n",
    "\n",
    "For the purposes of this notebook, we will instruct NeMo Curator to use 8 CPU workers. While most NeMo Curator functionalities can be executed on the CPU, some modules (such as semantic deduplication) can only be executed on the GPU. Please make sure to select the appropriate device.\n",
    "\n",
    "Note that you can increase or decrease the number of CPU workers depending on the runtime environment. Keep in mind that each CPU worker gets allocated a fixed amount of the total available system memory (RAM). Thus, if the environment does not have enough memory available, Dask operations might fail.\n",
    "\n",
    "Once we have decided on the resources to use, we can initialize our Dask cluster and start using NeMo Curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/node.py:187: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 33477 instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuDF Spilling is enabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-28 06:57:43,667 - distributed.scheduler - INFO - Retire worker addresses (0,)\n",
      "2024-10-28 06:57:43,673 - distributed.core - INFO - Received 'close-stream' from tcp://127.0.0.1:42632; closing.\n",
      "2024-10-28 06:57:43,674 - distributed.scheduler - INFO - Remove worker <WorkerState 'tcp://127.0.0.1:38093', name: 0, status: closing, memory: 0, processing: 0> (stimulus_id='handle-worker-cleanup-1730098663.6747935')\n",
      "2024-10-28 06:57:43,675 - distributed.scheduler - INFO - Lost all workers\n",
      "2024-10-28 06:57:45,112 - distributed.scheduler - INFO - Closing scheduler. Reason: unknown\n",
      "2024-10-28 06:57:45,113 - distributed.scheduler - INFO - Scheduler closing all comms\n"
     ]
    }
   ],
   "source": [
    "device = \"gpu\"  # It can be either \"cpu\" or \"gpu\"\n",
    "n_workers = 4  # Number of workers to use for Dask. If running out of memory, try reducing this.\n",
    "client = get_client(device, n_workers=n_workers, set_torch_to_use_rmm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## The Main Data Curation and Processing Pipeline\n",
    "\n",
    "We start by downloading and converting the dataset into a suitable format. This is done via the document builders that we have provided for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download directory:  /root/ODSC-Hackathon-Repository/data/raw\n",
      "File '/root/ODSC-Hackathon-Repository/data/raw/law-stackexchange-questions-answers.json' already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "dataset_df = download_and_convert_dataset(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_df['id'] = dataset_df['id'].apply(lambda x: x.split('-')[-1])\n",
    "dataset_df['id'] = dataset_df['id'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>question</th>\n",
       "      <th>question_score</th>\n",
       "      <th>answer</th>\n",
       "      <th>answer_score</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>law-stackexchange-questions-answers.json</td>\n",
       "      <td>94665</td>\n",
       "      <td>Why is drunk driving causing accident punished...</td>\n",
       "      <td>When people drink and drive and then cause an ...</td>\n",
       "      <td>23</td>\n",
       "      <td>Moral luck You have raised the issue of moral ...</td>\n",
       "      <td>72</td>\n",
       "      <td>criminal-law,driving,sentencing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>law-stackexchange-questions-answers.json</td>\n",
       "      <td>94671</td>\n",
       "      <td>What counts as consideration in contract law?</td>\n",
       "      <td>What counts as consideration in contract law? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>See generally Hamer v. Sidway (1891), 124 NY 5...</td>\n",
       "      <td>1</td>\n",
       "      <td>consideration,contract-law,legal-terms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>law-stackexchange-questions-answers.json</td>\n",
       "      <td>94683</td>\n",
       "      <td>Question Concerning Responding to Employer of ...</td>\n",
       "      <td>My high school daughter worked for about a yea...</td>\n",
       "      <td>1</td>\n",
       "      <td>Read the terms It’s quite likely that, if you ...</td>\n",
       "      <td>3</td>\n",
       "      <td>california,employment,teenager</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>law-stackexchange-questions-answers.json</td>\n",
       "      <td>67110</td>\n",
       "      <td>Can Hawaii secede from the U.S. through legal ...</td>\n",
       "      <td>Can Hawaii secede from the U.S. through legal ...</td>\n",
       "      <td>2</td>\n",
       "      <td>Currently, there is no legal means for a state...</td>\n",
       "      <td>9</td>\n",
       "      <td>constitutional-law,federalism,united-states</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>law-stackexchange-questions-answers.json</td>\n",
       "      <td>94678</td>\n",
       "      <td>Legality of privately bibby Stockholming to sa...</td>\n",
       "      <td>It seems that the principal impetus of moving ...</td>\n",
       "      <td>1</td>\n",
       "      <td>england-and-wales then what stops private citi...</td>\n",
       "      <td>1</td>\n",
       "      <td>any-jurisdiction,coast,law-of-the-sea,property...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   filename     id  \\\n",
       "0  law-stackexchange-questions-answers.json  94665   \n",
       "1  law-stackexchange-questions-answers.json  94671   \n",
       "2  law-stackexchange-questions-answers.json  94683   \n",
       "3  law-stackexchange-questions-answers.json  67110   \n",
       "4  law-stackexchange-questions-answers.json  94678   \n",
       "\n",
       "                                               title  \\\n",
       "0  Why is drunk driving causing accident punished...   \n",
       "1      What counts as consideration in contract law?   \n",
       "2  Question Concerning Responding to Employer of ...   \n",
       "3  Can Hawaii secede from the U.S. through legal ...   \n",
       "4  Legality of privately bibby Stockholming to sa...   \n",
       "\n",
       "                                            question  question_score  \\\n",
       "0  When people drink and drive and then cause an ...              23   \n",
       "1  What counts as consideration in contract law? ...               0   \n",
       "2  My high school daughter worked for about a yea...               1   \n",
       "3  Can Hawaii secede from the U.S. through legal ...               2   \n",
       "4  It seems that the principal impetus of moving ...               1   \n",
       "\n",
       "                                              answer  answer_score  \\\n",
       "0  Moral luck You have raised the issue of moral ...            72   \n",
       "1  See generally Hamer v. Sidway (1891), 124 NY 5...             1   \n",
       "2  Read the terms It’s quite likely that, if you ...             3   \n",
       "3  Currently, there is no legal means for a state...             9   \n",
       "4  england-and-wales then what stops private citi...             1   \n",
       "\n",
       "                                                tags  \n",
       "0                    criminal-law,driving,sentencing  \n",
       "1             consideration,contract-law,legal-terms  \n",
       "2                     california,employment,teenager  \n",
       "3        constitutional-law,federalism,united-states  \n",
       "4  any-jurisdiction,coast,law-of-the-sea,property...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "raw_dataset = DocumentDataset.from_pandas(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define our data curation pipeline. The pipeline we define here is very simple, as it contains basic filtering operations\n",
    "\n",
    "> NOTE: to use the modules that need a GPU, the dataset has to be converted to the `cudf` backend. Please refer to [this tutorial](https://github.com/NVIDIA/NeMo-Curator/tree/main/tutorials/peft-curation-with-sdg) for an example demonstrating the usage of GPU modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuotationUnifier(DocumentModifier):\n",
    "    def modify_document(self, text: str) -> str:\n",
    "        text = text.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        text = text.replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        text = text.replace(\"...\", '').replace(\"..\", '')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_curation_pipeline(dataset: DocumentDataset, device: str) -> DocumentDataset:\n",
    "    print(f\"Running curation pipeline on '{device}'...\")\n",
    "    orig_dataset = dataset\n",
    "    \n",
    "    def datacleaners(DocumentDataset) -> DocumentDataset:\n",
    "        cleaners = Sequential([\n",
    "            Modify(QuotationUnifier(), text_field=\"title\"),\n",
    "            Modify(QuotationUnifier(), text_field=\"question\"),\n",
    "            Modify(QuotationUnifier(), text_field=\"answer\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"title\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"question\"),\n",
    "            Modify(UnicodeReformatter(), text_field=\"answer\"),\n",
    "        ])\n",
    "        return cleaners(dataset)\n",
    "    \n",
    "    def datafilter(DocumentDataset) -> DocumentDataset:\n",
    "        filters = Sequential([\n",
    "            ScoreFilter(\n",
    "                WordCountFilter(min_words=50, max_words=1000, lang='en'),\n",
    "                text_field=\"question\",\n",
    "                score_type=int,\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                FilterLowScores(score_threshold=0),\n",
    "                text_field=\"question_score\",\n",
    "                score_type=bool,\n",
    "            ),\n",
    "            ScoreFilter(\n",
    "                FilterLowScores(score_threshold=0),\n",
    "                text_field=\"answer_score\",\n",
    "                score_type=bool,\n",
    "            ),\n",
    "\n",
    "        ])\n",
    "        return filters(dataset)\n",
    "\n",
    "    cpu_curation_steps = Sequential(\n",
    "        [\n",
    "            datacleaners,\n",
    "            datafilter\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Run the CPU curation steps.\n",
    "    dataset = cpu_curation_steps(dataset)\n",
    "    dataset = dataset.persist()\n",
    "    # Drop the columns that are no longer needed.\n",
    "    dataset.df = dataset.df.drop(columns=[\"answer\", \"answer_score\", \"question_score\"])\n",
    "    orig_len = len(orig_dataset.df)\n",
    "    new_len = len(dataset.df)\n",
    "\n",
    "    print(f\"Original dataset length: {orig_len}\")\n",
    "    print(f\"New dataset length: {new_len}\")\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to run the pipeline and get our final dataset. This may take up to 10 minutes to execute, especially if any GPU functionalities are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running curation pipeline on 'gpu'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 58.69 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 58.69 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset length: 24343\n",
      "New dataset length: 19164\n"
     ]
    }
   ],
   "source": [
    "curated_dataset = run_curation_pipeline(raw_dataset, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's specify the final columns that we would like our dataset to have. Depending on how you plan on consuming this dataset for training, you may decide to introduce other arbitrary columns to help the model learn better.\n",
    "\n",
    "Also, this is a great place to add system or instruction prompts to every record, in case you intend to use the same instruction prompt for every record.\n",
    "\n",
    "Let's define a function that formats the dataset, and also adds system prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def format_dataset(dataset: DocumentDataset, filename: str) -> DocumentDataset:\n",
    "    SYSTEM_PROMPT = \"Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\\n\\n\"\n",
    "\n",
    "    df = dataset.df.compute()\n",
    "    has_tags = \"tags\" in df.columns\n",
    "    df[\"input\"] = SYSTEM_PROMPT + \"TITLE:\\n\" + df[\"title\"] + \"\\n\\n\" + \"QUESTION:\\n\" + df[\"question\"]\n",
    "    df[\"output\"] = df[\"tags\"] if has_tags else \"\"  # If the dataset doesn't have tags, use an empty string.\n",
    "    df[\"filename\"] = filename\n",
    "\n",
    "    df = df.drop(columns=[\"title\", \"question\"])\n",
    "    if has_tags:\n",
    "        df = df.drop(columns=[\"tags\"]) # Drop the tags column if it exists.\n",
    "    return DocumentDataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the function above to format the dataset. We apply the same logic to the final evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset columns: Index(['filename', 'id', 'title', 'question', 'tags'], dtype='object')\n",
      "Formatted dataset columns: Index(['filename', 'id', 'input', 'output'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "formatted_dataset = format_dataset(curated_dataset, \"law-stackexchange-curated.jsonl\")\n",
    "print(f\"Original dataset columns: {curated_dataset.df.columns}\")\n",
    "print(f\"Formatted dataset columns: {formatted_dataset.df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the final dataset is ready, we can write it into a JSONL file that is in the format expected for training with NeMo Framework.\n",
    "\n",
    "> NOTE: The curated dataset will be written under `curator/data/curated_dataset/law-stackexchange-curated.jsonl`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated dataset columns: Index(['filename', 'id', 'input', 'output'], dtype='object')\n",
      "\n",
      "Saving curated dataset to '/root/ODSC-Hackathon-Repository/data/curated_dataset'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 22.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "print(f\"Curated dataset columns: {formatted_dataset.df.columns}\")\n",
    "result_fp = os.path.join(DATA_DIR, \"curated_dataset\")\n",
    "print()\n",
    "print(f\"Saving curated dataset to '{result_fp}'...\")\n",
    "formatted_dataset.to_json(result_fp, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Spliting the Dataset\n",
    "\n",
    "Before starting the model training procedure, let's split the dataset we've just curated into `training`, `validation` and `test` splits with 80/10/10 ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 22.90 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size: 19164\n",
      "After splitting:\n",
      "    Train size: 18206\n",
      "    Validation size: 958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/distributed/client.py:3361: UserWarning: Sending large graph of size 21.78 MiB.\n",
      "This may cause some slowdown.\n",
      "Consider loading the data with Dask directly\n",
      " or using futures or delayed objects to embed the data into the graph without repetition.\n",
      "See also https://docs.dask.org/en/stable/best-practices.html#load-data-with-dask for more information.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to disk complete for 1 partitions\n",
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "VAL_RATIO = 0.05\n",
    "\n",
    "df = formatted_dataset.df.compute()\n",
    "\n",
    "# Some sanity checks\n",
    "assert len(df) > 0, \"The dataset is empty.\"\n",
    "assert VAL_RATIO >= 0 and VAL_RATIO <= 1, \"VAL_RATIO must be between 0 and 1.\"\n",
    "val_size = int(len(df) * VAL_RATIO)\n",
    "output_dir = f\"{DATA_DIR}/split\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split the data into training and temporary sets\n",
    "train_df, val_df = train_test_split(df, test_size=val_size, random_state=42)\n",
    "\n",
    "print(f\"Original size: {len(df)}\")\n",
    "print(\"After splitting:\")\n",
    "print(f\"    Train size: {len(train_df)}\")\n",
    "print(f\"    Validation size: {len(val_df)}\")\n",
    "\n",
    "train_df[\"filename\"] = \"train.jsonl\"\n",
    "val_df[\"filename\"] = \"val.jsonl\"\n",
    "\n",
    "DocumentDataset.from_pandas(train_df).to_json(output_dir, write_to_filename=True)\n",
    "DocumentDataset.from_pandas(val_df).to_json(output_dir, write_to_filename=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Preparing the Submission Dataset\n",
    "\n",
    "The submission dataset is dataset of questions and titles, where every participating team would have to predict the tags for.\n",
    "It needs to have a format similar to training datasets so that you can evaluate your model on it, and submit your predicted tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 1 files\n",
      "Writing the formatted submission dataset to disk...\n",
      "Writing to disk complete for 1 partitions\n"
     ]
    }
   ],
   "source": [
    "submission_ds = \"data/submission/evaluation-dataset-verified-for-participants.jsonl\"\n",
    "assert os.path.exists(submission_ds), f\"The submission dataset does not exist at '{submission_ds}'\"\n",
    "submission_ds = DocumentDataset.read_json(submission_ds)\n",
    "submission_ds = format_dataset(submission_ds, \"submission.jsonl\")\n",
    "print(\"Writing the formatted submission dataset to disk...\")\n",
    "submission_ds.to_json(output_dir, write_to_filename=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have run the above cell, your data that is suitable for training will be written under `data/split`. When making submissions, run inference with your model on `data/split/submission.jsonl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Freeing Memory and Other Resources\n",
    "\n",
    "Before moving to the next notebook, please execute the following cell to free up all the allocated resources to avoid running into out-of-memory or other issues.\n",
    "\n",
    "Alternatively, please restart the kernel by navigating to `Kernel > Restart Kernel` (if using Jypyter notebook), or clicking the `Restart` button in VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client.close()\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
