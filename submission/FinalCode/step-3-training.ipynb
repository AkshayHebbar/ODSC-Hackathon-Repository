{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4c9fc31-b77e-4007-8901-d186024675cd",
   "metadata": {},
   "source": [
    "# Step 3: Training a LoRA Adapter\n",
    "\n",
    "This notebook performs the preparatory tasks needed for obtaining the base model that we will use for fine-tuning.\n",
    "\n",
    "This notebook showcases performing LoRA fine-tuning on the dataset that we curated in step 1.\n",
    "\n",
    "## Setup and Requirements\n",
    "Before proceeding, please make ensure you have completed the notebooks for steps 1 and 2. You will need to install one dependency to follow along. Execute the following cell before getting started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db9f6f9-b0c3-4c15-92d4-5c7ff0e5a286",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (1.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.10/dist-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a8e64b",
   "metadata": {},
   "source": [
    "Let's also specify the base model name that we will use for fine-tuning. This should be the same model you downloaded/converted in step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2328694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_to_use = \"google/gemma-2-2b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccfc187",
   "metadata": {},
   "source": [
    "---\n",
    "# Sanity Checking\n",
    "\n",
    "Let's do a quick sanity check to ensure we have all the pieces needed before moving forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11920d0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_MODEL=/root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "env: DATA_DIR=data/split\n",
      "env: TRAIN_DS=/root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "env: VAL_DS=/root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "env: RESULT_DIR=/root/ODSC-Hackathon-Repository/results\n",
      "\n",
      "################################################################################\n",
      "All checks passed. You are ready to go!\n",
      "    Base model file: /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "    Data directory: data/split\n",
      "    Results: /root/ODSC-Hackathon-Repository/results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "model_name = model_to_use.split('/')[-1].lower()\n",
    "\n",
    "# The path to the model checkpoint, and also the data directory containing the training, validation, and test data.\n",
    "nemo_model_fp = os.path.abspath(f\"models/{model_name}.nemo\")\n",
    "data_dir = \"data/split\"\n",
    "\n",
    "# The directory where the results will be stored.\n",
    "result_dir = os.path.abspath(\"results\")\n",
    "os.makedirs(result_dir, exist_ok=True)\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.exists(nemo_model_fp), f\"The model checkpoint at '{nemo_model_fp}' does not exist. Please ensure the model was downloaded successfully.\"\n",
    "assert os.path.exists(data_dir), f\"The data directory '{data_dir}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "train_fp = os.path.abspath(f\"{data_dir}/train.jsonl\")\n",
    "val_fp = os.path.abspath(f\"{data_dir}/val.jsonl\")\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.exists(train_fp), f\"The training data at '{train_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "assert os.path.exists(val_fp), f\"The validation data at '{val_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "#\n",
    "# Set the environment variables (needed for executing the next cell)\n",
    "#\n",
    "%env BASE_MODEL=$nemo_model_fp\n",
    "%env DATA_DIR=$data_dir\n",
    "%env TRAIN_DS=$train_fp\n",
    "%env VAL_DS=$val_fp\n",
    "%env RESULT_DIR=$result_dir\n",
    "\n",
    "print(f\"\\n{'#'*80}\")\n",
    "print(\"All checks passed. You are ready to go!\")\n",
    "print(f\"    Base model file: {nemo_model_fp}\")\n",
    "print(f\"    Data directory: {data_dir}\")\n",
    "print(f\"    Results: {result_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d658d64",
   "metadata": {},
   "source": [
    "---\n",
    "# Model Training\n",
    "\n",
    "With all the sanity checks passing, it is time to start model training.\n",
    "\n",
    "> NOTE: Running the following cell will remove any previously trained model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b4df27e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'data/split/*idx*': No such file or directory\n",
      "[NeMo W 2024-10-28 07:07:08 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:10 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-28 07:07:10 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 1000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 0.3\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /root/ODSC-Hackathon-Repository/results\n",
      "      exp_dir: /root/ODSC-Hackathon-Repository/results\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 10\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        chat: false\n",
      "        chat_prompt_tokens:\n",
      "          system_turn_start: \"\\0\"\n",
      "          turn_start: \"\\x11\"\n",
      "          label_start: \"\\x12\"\n",
      "          end_of_turn: '\n",
      "    \n",
      "            '\n",
      "          end_of_name: '\n",
      "    \n",
      "            '\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - /root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: true\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: '{input} {output}'\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - /root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names: null\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "      mcore_gpt: true\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:10 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:10 exp_manager:450] ExpManager schema\n",
      "[NeMo I 2024-10-28 07:07:10 exp_manager:451] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False, 'save_last_n_optim_states': -1}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'log_delta_step_timing': False, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-10-28 07:07:10 exp_manager:910] exp_manager received explicit_log_dir: /root/ODSC-Hackathon-Repository/results and at least one of exp_dir: /root/ODSC-Hackathon-Repository/results, or version: None. Please note that exp_dir, name, and version will be ignored.\n",
      "[NeMo W 2024-10-28 07:07:10 exp_manager:837] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/root/ODSC-Hackathon-Repository/results/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:10 exp_manager:509] Experiments will be logged at /root/ODSC-Hackathon-Repository/results\n",
      "[NeMo I 2024-10-28 07:07:10 exp_manager:1063] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:10 exp_manager:1201] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 1000. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:10 exp_manager:646] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:16 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:339] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:348] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:402] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:07:16 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-10-28 07:07:16 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "[NeMo I 2024-10-28 07:07:16 tokenizer_utils:197] Getting SentencePiece with model: /tmp/tmp7bz6ruz2/7785645eb8594f67b5e7f32b1fee7d65_tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:16 megatron_base_model:604] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:07:16 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:28 nlp_overrides:1374] Model MegatronGPTSFTModel was successfully restored from /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo.\n",
      "[NeMo I 2024-10-28 07:07:28 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-10-28 07:07:28 nlp_adapter_mixins:245] Before adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 2.6 B  | train\n",
      "    ------------------------------------------------\n",
      "    0         Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,457.368Total estimated model params size (MB)\n",
      "    452       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 07:07:31 nlp_adapter_mixins:250] After adding PEFT params:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 2.6 B  | train\n",
      "    ------------------------------------------------\n",
      "    5.3 M     Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    582       Modules in train mode\n",
      "    0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:31 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-28 07:07:31 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:32 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:494] Building indexing for fn = /root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:506] Saving idx file = /root/ODSC-Hackathon-Repository/data/split/val.jsonl.idx.npy\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:508] Saving metadata file = /root/ODSC-Hackathon-Repository/data/split/val.jsonl.idx.info\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.101332\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.089192\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:249] Loading /root/ODSC-Hackathon-Repository/data/split/val.jsonl\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001002\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-28 07:07:32 megatron_gpt_sft_model:840] Length of val dataset: 958\n",
      "[NeMo I 2024-10-28 07:07:32 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:494] Building indexing for fn = /root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:506] Saving idx file = /root/ODSC-Hackathon-Repository/data/split/train.jsonl.idx.npy\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:508] Saving metadata file = /root/ODSC-Hackathon-Repository/data/split/train.jsonl.idx.info\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.186823\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:528] Processing 1 data files using 2 workers\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.095602\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:249] Loading /root/ODSC-Hackathon-Repository/data/split/train.jsonl\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000707\n",
      "[NeMo I 2024-10-28 07:07:32 text_memmap_dataset:165] Computing global indices\n",
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-10-28 07:07:32 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.07 (sec)\n",
      "[NeMo I 2024-10-28 07:07:32 megatron_gpt_sft_model:849] Length of train dataset: 10050\n",
      "[NeMo I 2024-10-28 07:07:32 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-10-28 07:07:32 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-10-28 07:07:32 megatron_base_model:1230] Ignoring `trainer.max_epochs` when computing `max_steps` because `trainer.max_steps` is already set to 1000.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-10-28 07:07:32 nlp_adapter_mixins:329] Optimizer groups set:\n",
      "      | Name  | Type          | Params | Mode \n",
      "    ------------------------------------------------\n",
      "    0 | model | Float16Module | 2.6 B  | train\n",
      "    ------------------------------------------------\n",
      "    5.3 M     Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    582       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 07:07:32 modelPT:787] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-10-28 07:07:32 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f65f46b33a0>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n",
      "[NeMo I 2024-10-28 07:07:32 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f65f46b3b80>\" \n",
      "    will be used during training (effective maximum steps = 1000) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 1000\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 2.6 B  | train\n",
      "------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "2.6 B     Non-trainable params\n",
      "2.6 B     Total params\n",
      "10,478.667Total estimated model params size (MB)\n",
      "582       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[NeMo W 2024-10-28 07:07:33 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-10-28 07:07:33 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "Sanity Checking DataLoader 0: 100%|| 2/2 [00:02<00:00,  0.76it/s][NeMo I 2024-10-28 07:07:35 num_microbatches_calculator:228] setting number of microbatches to constant 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:07:35 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 07:07:35 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 07:07:35 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 07:07:35 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  20%|        | 200/1000 [05:56<23:45, reduced_train_loss=1.150, global_step=199.0, consumed_samples=2e+3, train_step_timing in s=1.800]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 07:13:32 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n",
      "Validation:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|          | 1/96 [00:00<01:27,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 2/96 [00:01<01:27,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 3/96 [00:02<01:29,  1.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|         | 4/96 [00:03<01:27,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 5/96 [00:04<01:26,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 6/96 [00:05<01:26,  1.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|         | 7/96 [00:06<01:25,  1.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 8/96 [00:07<01:24,  1.04it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 9/96 [00:08<01:22,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|         | 10/96 [00:09<01:21,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|        | 11/96 [00:10<01:20,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 12/96 [00:11<01:20,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 13/96 [00:12<01:19,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 14/96 [00:13<01:18,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|        | 15/96 [00:14<01:17,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 16/96 [00:15<01:15,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 17/96 [00:16<01:14,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|        | 18/96 [00:17<01:13,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 19/96 [00:17<01:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 20/96 [00:18<01:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 21/96 [00:19<01:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 22/96 [00:20<01:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 23/96 [00:21<01:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|       | 24/96 [00:22<01:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 25/96 [00:23<01:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 26/96 [00:24<01:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 27/96 [00:25<01:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 28/96 [00:26<01:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 29/96 [00:27<01:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|      | 30/96 [00:28<01:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 31/96 [00:29<01:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 32/96 [00:30<01:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|      | 33/96 [00:31<00:59,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 34/96 [00:32<00:58,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 35/96 [00:33<00:57,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 36/96 [00:34<00:56,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 37/96 [00:35<00:55,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|      | 38/96 [00:35<00:54,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 39/96 [00:36<00:53,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 40/96 [00:37<00:53,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|     | 41/96 [00:38<00:52,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 42/96 [00:39<00:51,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 43/96 [00:40<00:50,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|     | 44/96 [00:41<00:49,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 45/96 [00:42<00:48,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 46/96 [00:43<00:47,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|     | 47/96 [00:44<00:46,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 48/96 [00:45<00:45,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|     | 49/96 [00:46<00:44,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 50/96 [00:47<00:43,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 51/96 [00:48<00:42,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|    | 52/96 [00:49<00:41,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 53/96 [00:50<00:40,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 54/96 [00:51<00:39,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|    | 55/96 [00:52<00:38,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 56/96 [00:52<00:37,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 57/96 [00:53<00:36,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|    | 58/96 [00:54<00:35,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|   | 59/96 [00:55<00:35,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 60/96 [00:56<00:34,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 61/96 [00:57<00:33,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 62/96 [00:58<00:32,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|   | 63/96 [00:59<00:31,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 64/96 [01:00<00:30,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 65/96 [01:01<00:29,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|   | 66/96 [01:02<00:28,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 67/96 [01:03<00:27,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 68/96 [01:04<00:26,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 69/96 [01:05<00:25,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 70/96 [01:06<00:24,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 71/96 [01:07<00:23,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|  | 72/96 [01:08<00:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 73/96 [01:09<00:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 74/96 [01:10<00:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 75/96 [01:11<00:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 76/96 [01:11<00:18,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 77/96 [01:12<00:17,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%| | 78/96 [01:13<00:17,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 79/96 [01:14<00:16,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 80/96 [01:15<00:15,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%| | 81/96 [01:16<00:14,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 82/96 [01:17<00:13,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 83/96 [01:18<00:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 84/96 [01:19<00:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 85/96 [01:20<00:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 86/96 [01:21<00:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 87/96 [01:22<00:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 88/96 [01:23<00:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|| 89/96 [01:24<00:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 90/96 [01:25<00:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 91/96 [01:26<00:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 92/96 [01:26<00:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 93/96 [01:27<00:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 94/96 [01:28<00:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 95/96 [01:29<00:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 96/96 [01:30<00:00,  1.06it/s]\u001b[A[NeMo I 2024-10-28 07:15:02 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.100\n",
      "Epoch 0, global step 200: 'validation_loss' reached 1.10000 (best 1.10000), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.100-step=200-consumed_samples=2000.0.ckpt' as top 1\n",
      "[NeMo W 2024-10-28 07:15:03 nlp_overrides:625] DistributedCheckpointIO configured but should not be used. Reverting back to TorchCheckpointIO\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|      | 400/1000 [13:23<20:05, reduced_train_loss=1.160, global_step=399.0, consumed_samples=4e+3, train_step_timing in s=1.740, val_loss=1.100]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 07:20:59 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n",
      "Validation:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|          | 1/96 [00:00<01:27,  1.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 2/96 [00:01<01:26,  1.09it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 3/96 [00:02<01:25,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|         | 4/96 [00:03<01:25,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 5/96 [00:04<01:24,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 6/96 [00:05<01:24,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|         | 7/96 [00:06<01:23,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 8/96 [00:07<01:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 9/96 [00:08<01:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|         | 10/96 [00:09<01:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|        | 11/96 [00:10<01:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 12/96 [00:11<01:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 13/96 [00:12<01:18,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 14/96 [00:13<01:17,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|        | 15/96 [00:14<01:16,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 16/96 [00:15<01:15,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 17/96 [00:15<01:14,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|        | 18/96 [00:16<01:13,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 19/96 [00:17<01:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 20/96 [00:18<01:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 21/96 [00:19<01:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 22/96 [00:20<01:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 23/96 [00:21<01:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|       | 24/96 [00:22<01:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 25/96 [00:23<01:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 26/96 [00:24<01:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 27/96 [00:25<01:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 28/96 [00:26<01:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 29/96 [00:27<01:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|      | 30/96 [00:28<01:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 31/96 [00:29<01:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 32/96 [00:30<01:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|      | 33/96 [00:31<00:59,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 34/96 [00:32<00:58,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 35/96 [00:33<00:57,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 36/96 [00:33<00:56,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 37/96 [00:34<00:55,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|      | 38/96 [00:35<00:54,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 39/96 [00:36<00:53,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 40/96 [00:37<00:52,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|     | 41/96 [00:38<00:51,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 42/96 [00:39<00:51,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 43/96 [00:40<00:50,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|     | 44/96 [00:41<00:49,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 45/96 [00:42<00:48,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 46/96 [00:43<00:47,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|     | 47/96 [00:44<00:46,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 48/96 [00:45<00:45,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|     | 49/96 [00:46<00:44,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 50/96 [00:47<00:43,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 51/96 [00:48<00:42,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|    | 52/96 [00:49<00:41,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 53/96 [00:50<00:40,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 54/96 [00:51<00:39,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|    | 55/96 [00:52<00:38,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 56/96 [00:53<00:37,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 57/96 [00:54<00:36,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|    | 58/96 [00:55<00:36,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|   | 59/96 [00:55<00:35,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 60/96 [00:56<00:34,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 61/96 [00:57<00:33,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 62/96 [00:58<00:32,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|   | 63/96 [00:59<00:31,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 64/96 [01:00<00:30,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 65/96 [01:01<00:29,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|   | 66/96 [01:02<00:28,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 67/96 [01:03<00:27,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 68/96 [01:04<00:26,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 69/96 [01:05<00:25,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 70/96 [01:06<00:24,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 71/96 [01:07<00:23,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|  | 72/96 [01:08<00:22,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 73/96 [01:09<00:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 74/96 [01:10<00:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 75/96 [01:11<00:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 76/96 [01:12<00:18,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 77/96 [01:12<00:18,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%| | 78/96 [01:13<00:17,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 79/96 [01:14<00:16,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 80/96 [01:15<00:15,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%| | 81/96 [01:16<00:14,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 82/96 [01:17<00:13,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 83/96 [01:18<00:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 84/96 [01:19<00:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 85/96 [01:20<00:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 86/96 [01:21<00:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 87/96 [01:22<00:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 88/96 [01:23<00:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|| 89/96 [01:24<00:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 90/96 [01:25<00:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 91/96 [01:26<00:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 92/96 [01:27<00:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 93/96 [01:28<00:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 94/96 [01:28<00:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 95/96 [01:29<00:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 96/96 [01:30<00:00,  1.06it/s]\u001b[A[NeMo I 2024-10-28 07:22:30 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.071 >= min_delta = 0.001. New best score: 1.029\n",
      "Epoch 0, global step 400: 'validation_loss' reached 1.02901 (best 1.02901), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.029-step=400-consumed_samples=4000.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  40%|      | 400/1000 [14:54<22:21, reduced_train_loss=1.160, global_step=399.0, consumed_samples=4e+3, train_step_timing in s=1.740, val_loss=1.030][NeMo I 2024-10-28 07:22:30 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.100-step=200-consumed_samples=2000.0.ckpt\n",
      "[NeMo I 2024-10-28 07:22:31 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.100-step=200-consumed_samples=2000.0-last.ckpt\n",
      "Epoch 0: :  60%|    | 600/1000 [20:51<13:54, reduced_train_loss=0.686, global_step=599.0, consumed_samples=6e+3, train_step_timing in s=1.770, val_loss=1.030]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 07:28:26 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n",
      "Validation:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|          | 1/96 [00:00<01:32,  1.02it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 2/96 [00:01<01:29,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 3/96 [00:02<01:28,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|         | 4/96 [00:03<01:26,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 5/96 [00:04<01:25,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 6/96 [00:05<01:24,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|         | 7/96 [00:06<01:23,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 8/96 [00:07<01:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 9/96 [00:08<01:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|         | 10/96 [00:09<01:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|        | 11/96 [00:10<01:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 12/96 [00:11<01:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 13/96 [00:12<01:18,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 14/96 [00:13<01:17,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|        | 15/96 [00:14<01:16,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 16/96 [00:15<01:15,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 17/96 [00:16<01:14,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|        | 18/96 [00:17<01:13,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 19/96 [00:17<01:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 20/96 [00:18<01:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 21/96 [00:19<01:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 22/96 [00:20<01:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 23/96 [00:21<01:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|       | 24/96 [00:22<01:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 25/96 [00:23<01:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 26/96 [00:24<01:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 27/96 [00:25<01:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 28/96 [00:26<01:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 29/96 [00:27<01:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|      | 30/96 [00:28<01:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 31/96 [00:29<01:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 32/96 [00:30<01:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|      | 33/96 [00:31<00:59,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 34/96 [00:32<00:58,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 35/96 [00:33<00:57,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 36/96 [00:34<00:56,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 37/96 [00:35<00:55,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|      | 38/96 [00:35<00:54,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 39/96 [00:36<00:54,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 40/96 [00:37<00:53,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|     | 41/96 [00:38<00:52,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 42/96 [00:39<00:51,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 43/96 [00:40<00:50,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|     | 44/96 [00:41<00:49,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 45/96 [00:42<00:48,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 46/96 [00:43<00:47,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|     | 47/96 [00:44<00:46,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 48/96 [00:45<00:45,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|     | 49/96 [00:46<00:44,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 50/96 [00:47<00:43,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 51/96 [00:48<00:42,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|    | 52/96 [00:49<00:41,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 53/96 [00:50<00:40,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 54/96 [00:51<00:39,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|    | 55/96 [00:52<00:38,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 56/96 [00:52<00:37,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 57/96 [00:53<00:36,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|    | 58/96 [00:54<00:35,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|   | 59/96 [00:55<00:35,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 60/96 [00:56<00:34,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 61/96 [00:57<00:33,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 62/96 [00:58<00:32,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|   | 63/96 [00:59<00:31,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 64/96 [01:00<00:30,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 65/96 [01:01<00:29,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|   | 66/96 [01:02<00:28,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 67/96 [01:03<00:27,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 68/96 [01:04<00:26,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 69/96 [01:05<00:25,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 70/96 [01:06<00:24,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 71/96 [01:07<00:23,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|  | 72/96 [01:08<00:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 73/96 [01:09<00:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 74/96 [01:10<00:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 75/96 [01:11<00:19,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 76/96 [01:12<00:18,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 77/96 [01:13<00:18,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%| | 78/96 [01:13<00:17,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 79/96 [01:14<00:16,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 80/96 [01:15<00:15,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%| | 81/96 [01:16<00:14,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 82/96 [01:17<00:13,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 83/96 [01:18<00:12,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 84/96 [01:19<00:11,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 85/96 [01:20<00:10,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 86/96 [01:21<00:09,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 87/96 [01:22<00:08,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 88/96 [01:23<00:07,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|| 89/96 [01:24<00:06,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 90/96 [01:25<00:05,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 91/96 [01:26<00:04,  1.05it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 92/96 [01:27<00:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 93/96 [01:28<00:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 94/96 [01:29<00:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 95/96 [01:29<00:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 96/96 [01:30<00:00,  1.06it/s]\u001b[A[NeMo I 2024-10-28 07:29:57 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.048 >= min_delta = 0.001. New best score: 0.981\n",
      "Epoch 0, global step 600: 'validation_loss' reached 0.98120 (best 0.98120), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.981-step=600-consumed_samples=6000.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  60%|    | 600/1000 [22:22<14:54, reduced_train_loss=0.686, global_step=599.0, consumed_samples=6e+3, train_step_timing in s=1.770, val_loss=0.981][NeMo I 2024-10-28 07:29:58 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.029-step=400-consumed_samples=4000.0.ckpt\n",
      "[NeMo I 2024-10-28 07:29:58 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.029-step=400-consumed_samples=4000.0-last.ckpt\n",
      "Epoch 0: :  80%|  | 800/1000 [28:18<07:04, reduced_train_loss=0.961, global_step=799.0, consumed_samples=8e+3, train_step_timing in s=1.770, val_loss=0.981]  \n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-10-28 07:35:53 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n",
      "Validation:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   1%|          | 1/96 [00:00<01:28,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   2%|         | 2/96 [00:01<01:27,  1.08it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|         | 3/96 [00:02<01:26,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   4%|         | 4/96 [00:03<01:25,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   5%|         | 5/96 [00:04<01:25,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|         | 6/96 [00:05<01:24,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   7%|         | 7/96 [00:06<01:23,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:   8%|         | 8/96 [00:07<01:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|         | 9/96 [00:08<01:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  10%|         | 10/96 [00:09<01:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  11%|        | 11/96 [00:10<01:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|        | 12/96 [00:11<01:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  14%|        | 13/96 [00:12<01:18,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|        | 14/96 [00:13<01:17,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  16%|        | 15/96 [00:14<01:16,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  17%|        | 16/96 [00:15<01:15,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|        | 17/96 [00:16<01:14,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  19%|        | 18/96 [00:16<01:13,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  20%|        | 19/96 [00:17<01:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|        | 20/96 [00:18<01:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  22%|       | 21/96 [00:19<01:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  23%|       | 22/96 [00:20<01:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|       | 23/96 [00:21<01:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  25%|       | 24/96 [00:22<01:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  26%|       | 25/96 [00:23<01:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|       | 26/96 [00:24<01:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  28%|       | 27/96 [00:25<01:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  29%|       | 28/96 [00:26<01:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|       | 29/96 [00:27<01:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  31%|      | 30/96 [00:28<01:01,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  32%|      | 31/96 [00:29<01:01,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|      | 32/96 [00:30<01:00,  1.07it/s]\u001b[A\n",
      "Validation DataLoader 0:  34%|      | 33/96 [00:30<00:59,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  35%|      | 34/96 [00:31<00:58,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|      | 35/96 [00:32<00:57,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  38%|      | 36/96 [00:33<00:56,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|      | 37/96 [00:34<00:55,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  40%|      | 38/96 [00:35<00:54,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  41%|      | 39/96 [00:36<00:53,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|     | 40/96 [00:37<00:52,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  43%|     | 41/96 [00:38<00:51,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  44%|     | 42/96 [00:39<00:50,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|     | 43/96 [00:40<00:49,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  46%|     | 44/96 [00:41<00:48,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  47%|     | 45/96 [00:42<00:47,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|     | 46/96 [00:43<00:47,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  49%|     | 47/96 [00:44<00:46,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|     | 48/96 [00:45<00:45,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  51%|     | 49/96 [00:46<00:44,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|    | 50/96 [00:46<00:43,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  53%|    | 51/96 [00:47<00:42,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  54%|    | 52/96 [00:48<00:41,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|    | 53/96 [00:49<00:40,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  56%|    | 54/96 [00:50<00:39,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  57%|    | 55/96 [00:51<00:38,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|    | 56/96 [00:52<00:37,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  59%|    | 57/96 [00:53<00:36,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  60%|    | 58/96 [00:54<00:35,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|   | 59/96 [00:55<00:34,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  62%|   | 60/96 [00:56<00:33,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|   | 61/96 [00:57<00:32,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  65%|   | 62/96 [00:58<00:31,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  66%|   | 63/96 [00:59<00:31,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|   | 64/96 [01:00<00:30,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  68%|   | 65/96 [01:01<00:29,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  69%|   | 66/96 [01:02<00:28,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|   | 67/96 [01:02<00:27,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  71%|   | 68/96 [01:03<00:26,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  72%|  | 69/96 [01:04<00:25,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|  | 70/96 [01:05<00:24,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  74%|  | 71/96 [01:06<00:23,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  75%|  | 72/96 [01:07<00:22,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|  | 73/96 [01:08<00:21,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  77%|  | 74/96 [01:09<00:20,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  78%|  | 75/96 [01:10<00:19,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|  | 76/96 [01:11<00:18,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  80%|  | 77/96 [01:12<00:17,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  81%| | 78/96 [01:13<00:16,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%| | 79/96 [01:14<00:16,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  83%| | 80/96 [01:15<00:15,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  84%| | 81/96 [01:16<00:14,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%| | 82/96 [01:17<00:13,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  86%| | 83/96 [01:18<00:12,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%| | 84/96 [01:19<00:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 85/96 [01:20<00:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 86/96 [01:20<00:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 87/96 [01:21<00:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 88/96 [01:22<00:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|| 89/96 [01:23<00:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 90/96 [01:24<00:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 91/96 [01:25<00:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 92/96 [01:26<00:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 93/96 [01:27<00:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 94/96 [01:28<00:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 95/96 [01:29<00:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 96/96 [01:30<00:00,  1.06it/s]\u001b[A[NeMo I 2024-10-28 07:37:24 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.027 >= min_delta = 0.001. New best score: 0.954\n",
      "Epoch 0, global step 800: 'validation_loss' reached 0.95373 (best 0.95373), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.954-step=800-consumed_samples=8000.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  80%|  | 800/1000 [29:48<07:27, reduced_train_loss=0.961, global_step=799.0, consumed_samples=8e+3, train_step_timing in s=1.770, val_loss=0.954][NeMo I 2024-10-28 07:37:24 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.981-step=600-consumed_samples=6000.0.ckpt\n",
      "[NeMo I 2024-10-28 07:37:24 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.981-step=600-consumed_samples=6000.0-last.ckpt\n",
      "Validation DataLoader 0:  86%| | 83/96 [01:17<00:12,  1.06it/s]\u001b[A, global_step=976.0, consumed_samples=9770.0, train_step_timing in s=1.790, val_loss=0.954]\n",
      "Validation DataLoader 0:  88%| | 84/96 [01:18<00:11,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  89%| | 85/96 [01:19<00:10,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  90%| | 86/96 [01:20<00:09,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%| | 87/96 [01:21<00:08,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  92%|| 88/96 [01:22<00:07,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  93%|| 89/96 [01:23<00:06,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|| 90/96 [01:24<00:05,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  95%|| 91/96 [01:25<00:04,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  96%|| 92/96 [01:26<00:03,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|| 93/96 [01:27<00:02,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  98%|| 94/96 [01:28<00:01,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0:  99%|| 95/96 [01:29<00:00,  1.06it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|| 96/96 [01:30<00:00,  1.06it/s]\u001b[A[NeMo I 2024-10-28 07:44:50 num_microbatches_calculator:228] setting number of microbatches to constant 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.005 >= min_delta = 0.001. New best score: 0.949\n",
      "Epoch 0, global step 1000: 'validation_loss' reached 0.94917 (best 0.94917), saving model to '/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.949-step=1000-consumed_samples=10000.0.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|| 1000/1000 [37:14<00:00, reduced_train_loss=0.876, global_step=999.0, consumed_samples=1e+4, train_step_timing in s=1.840, val_loss=0.949][NeMo I 2024-10-28 07:44:50 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.954-step=800-consumed_samples=8000.0.ckpt\n",
      "[NeMo I 2024-10-28 07:44:51 nlp_overrides:609] Removing checkpoint: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.954-step=800-consumed_samples=8000.0-last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=1000` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|| 1000/1000 [37:15<00:00, reduced_train_loss=0.876, global_step=999.0, consumed_samples=1e+4, train_step_timing in s=1.840, val_loss=0.949]\n",
      "[NeMo I 2024-10-28 07:44:51 perf_metrics:87] TFLOPs per sec per GPU=-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-10-28 07:44:51 perf_metrics:85] Failed to calculate TFLOPs per sec per GPU.\n",
      "    FLOPs measurement not supported for finetuning jobs\n",
      "Restoring states from the checkpoint path at /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.949-step=1000-consumed_samples=10000.0.ckpt\n",
      "Restored all states from the checkpoint at /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=0.949-step=1000-consumed_samples=10000.0.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Clear up cached mem-map file\n",
    "rm $DATA_DIR/*idx*\n",
    "# Clean up prior results\n",
    "rm -r $RESULT_DIR\n",
    "\n",
    "torchrun --nproc_per_node=1 \\\n",
    "/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "    exp_manager.exp_dir=${RESULT_DIR} \\\n",
    "    exp_manager.explicit_log_dir=${RESULT_DIR} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.val_check_interval=200 \\\n",
    "    trainer.max_steps=1000 \\\n",
    "    trainer.gradient_clip_val=0.3 \\\n",
    "    model.megatron_amp_O2=True \\\n",
    "    ++model.mcore_gpt=True \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.micro_batch_size=1 \\\n",
    "    model.global_batch_size=10 \\\n",
    "    model.restore_from_path=${BASE_MODEL} \\\n",
    "    model.data.train_ds.num_workers=0 \\\n",
    "    model.data.train_ds.add_bos=True \\\n",
    "    model.data.validation_ds.num_workers=0 \\\n",
    "    model.data.train_ds.file_names=[${TRAIN_DS}] \\\n",
    "    model.data.train_ds.concat_sampling_probabilities=[1.0] \\\n",
    "    model.data.validation_ds.file_names=[${VAL_DS}] \\\n",
    "    model.peft.peft_scheme=${SCHEME}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0a1d15",
   "metadata": {},
   "source": [
    "---\n",
    "# Inference and Submission\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e1b57",
   "metadata": {},
   "source": [
    "To make a submission, run inference with your model on the test dataset at `data/split/submission.jsonl`.\n",
    "\n",
    "> NOTE: This dataset was generated as part of Step 1. Please ensure it exists before proceeding.\n",
    "\n",
    "In order to do this, set the variable pointing to your submission data file in the set below, then excute the final cell.\n",
    "\n",
    "The inference results will be written under `results/inference` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c8569f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference set: /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "Trained adapter: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "env: TEST_DS=/root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "env: TEST_FP=submission.jsonl\n",
      "env: TRAINED_ADAPTER=/root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n"
     ]
    }
   ],
   "source": [
    "test_fp = os.path.abspath(f\"{data_dir}/submission.jsonl\")\n",
    "assert os.path.exists(test_fp), f\"The submission data at '{test_fp}' does not exist. Please ensure the data was prepared successfully.\"\n",
    "\n",
    "test_fp = os.path.abspath(test_fp)\n",
    "adapter_fp = f\"{result_dir}/checkpoints/megatron_gpt_peft_lora_tuning.nemo\"\n",
    "os.makedirs(f\"{result_dir}/inference\", exist_ok=True)\n",
    "\n",
    "print(f\"Inference set: {test_fp}\")\n",
    "print(f\"Trained adapter: {adapter_fp}\")\n",
    "test_filename = os.path.basename(test_fp)\n",
    "\n",
    "\n",
    "%env TEST_DS=$test_fp\n",
    "%env TEST_FP=$test_filename\n",
    "%env TRAINED_ADAPTER=$adapter_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "445e222c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:53:14 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:53:16 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-10-28 07:53:16 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 1\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 1\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: /root/ODSC-Hackathon-Repository/results/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "          names:\n",
      "          - infer\n",
      "          global_batch_size: 16\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: ${data.train_ds.label_key}\n",
      "          add_eos: ${data.train_ds.add_eos}\n",
      "          add_sep: ${data.train_ds.add_sep}\n",
      "          add_bos: ${data.train_ds.add_bos}\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: results/inference/infer-submission.jsonl\n",
      "          truncation_field: ${data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:53:16 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:53:22 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:339] All context parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:348] All model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:402] All embedding group ranks: [[0]]\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:53:22 tokenizer_utils:197] Getting SentencePiece with model: /tmp/tmp6r3lmtsl/7785645eb8594f67b5e7f32b1fee7d65_tokenizer.model\n",
      "[NeMo I 2024-10-28 07:53:22 megatron_base_model:604] Padded vocab_size: 256000, original vocab_size: 256000, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bootstrap_backend in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:1189] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: multi_latent_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_intermediate_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_shared_expert_overlap in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: cp_comm_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-10-28 07:53:22 megatron_base_model:577] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 1 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:53:38 nlp_overrides:1374] Model MegatronGPTSFTModel was successfully restored from /root/ODSC-Hackathon-Repository/models/gemma-2-2b.nemo.\n",
      "[NeMo I 2024-10-28 07:53:38 nlp_adapter_mixins:245] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 2.6 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,457.368Total estimated model params size (MB)\n",
      "    451       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 07:53:41 nlp_adapter_mixins:250] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 2.6 B  | train\n",
      "    -------------------------------------------\n",
      "    5.3 M     Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    581       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-10-28 07:53:41 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 2.6 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    2.6 B     Non-trainable params\n",
      "    2.6 B     Total params\n",
      "    10,478.667Total estimated model params size (MB)\n",
      "    0         Modules in train mode\n",
      "    581       Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 07:53:41 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-10-28 07:53:41 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-10-28 07:53:41 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:528] Processing 1 data files using 6 workers\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:494] Building indexing for fn = /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:506] Saving idx file = /root/ODSC-Hackathon-Repository/data/split/submission.jsonl.idx.npy\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:508] Saving metadata file = /root/ODSC-Hackathon-Repository/data/split/submission.jsonl.idx.info\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:543] Time building 1 / 1 mem-mapped files: 0:00:00.230623\n",
      "[NeMo I 2024-10-28 07:53:41 text_memmap_dataset:528] Processing 1 data files using 6 workers\n",
      "[NeMo I 2024-10-28 07:53:42 text_memmap_dataset:543] Time building 0 / 1 mem-mapped files: 0:00:00.191846\n",
      "[NeMo I 2024-10-28 07:53:42 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-10-28 07:53:42 text_memmap_dataset:249] Loading /root/ODSC-Hackathon-Repository/data/split/submission.jsonl\n",
      "[NeMo I 2024-10-28 07:53:42 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001194\n",
      "[NeMo I 2024-10-28 07:53:42 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-10-28 07:53:42 megatron_gpt_sft_model:831] Length of test dataset: 5000\n",
      "[NeMo I 2024-10-28 07:53:42 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "[NeMo W 2024-10-28 07:53:42 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   0%|          | 0/313 [00:00<?, ?it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   0%|          | 1/313 [00:13<1:07:46,  0.08it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   1%|          | 2/313 [00:27<1:10:19,  0.07it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   1%|          | 3/313 [00:37<1:05:17,  0.08it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   1%|         | 4/313 [00:49<1:03:43,  0.08it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   2%|         | 5/313 [00:59<1:01:26,  0.08it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   2%|         | 6/313 [01:08<58:47,  0.09it/s]  setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   2%|         | 7/313 [01:17<56:17,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   3%|         | 8/313 [01:27<55:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   3%|         | 9/313 [01:35<53:55,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   3%|         | 10/313 [01:47<54:13,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   4%|         | 11/313 [01:59<54:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   4%|         | 12/313 [02:09<54:04,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   4%|         | 13/313 [02:21<54:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   4%|         | 14/313 [02:33<54:39,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   5%|         | 15/313 [02:44<54:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   5%|         | 16/313 [02:57<54:59,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   5%|         | 17/313 [03:07<54:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   6%|         | 18/313 [03:18<54:06,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   6%|         | 19/313 [03:27<53:24,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   6%|         | 20/313 [03:34<52:15,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   7%|         | 21/313 [03:47<52:45,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   7%|         | 22/313 [03:59<52:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   7%|         | 23/313 [04:11<52:46,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   8%|         | 24/313 [04:22<52:41,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   8%|         | 25/313 [04:29<51:48,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   8%|         | 26/313 [04:39<51:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   9%|         | 27/313 [04:53<51:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   9%|         | 28/313 [05:06<52:02,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:   9%|         | 29/313 [05:17<51:50,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  10%|         | 30/313 [05:30<51:53,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  10%|         | 31/313 [05:40<51:37,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  10%|         | 32/313 [05:53<51:45,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  11%|         | 33/313 [06:02<51:11,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  11%|         | 34/313 [06:12<50:56,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  11%|         | 35/313 [06:24<50:53,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  12%|        | 36/313 [06:38<51:05,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  12%|        | 37/313 [06:48<50:43,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  12%|        | 38/313 [06:57<50:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  12%|        | 39/313 [07:06<49:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  13%|        | 40/313 [07:21<50:13,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  13%|        | 41/313 [07:37<50:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  13%|        | 42/313 [07:47<50:18,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  14%|        | 43/313 [07:59<50:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  14%|        | 44/313 [08:12<50:10,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  14%|        | 45/313 [08:24<50:06,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  15%|        | 46/313 [08:35<49:50,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  15%|        | 47/313 [08:45<49:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  15%|        | 48/313 [08:59<49:37,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  16%|        | 49/313 [09:13<49:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  16%|        | 50/313 [09:24<49:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  16%|        | 51/313 [09:39<49:38,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  17%|        | 52/313 [09:51<49:28,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  17%|        | 53/313 [10:05<49:28,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  17%|        | 54/313 [10:15<49:11,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  18%|        | 55/313 [10:23<48:47,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  18%|        | 56/313 [10:35<48:35,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  18%|        | 57/313 [10:43<48:11,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  19%|        | 58/313 [10:50<47:40,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  19%|        | 59/313 [10:58<47:13,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  19%|        | 60/313 [11:09<47:02,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  19%|        | 61/313 [11:24<47:07,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  20%|        | 62/313 [11:32<46:44,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  20%|        | 63/313 [11:40<46:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  20%|        | 64/313 [11:50<46:04,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  21%|        | 65/313 [12:02<45:56,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  21%|        | 66/313 [12:10<45:34,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  21%|       | 67/313 [12:18<45:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  22%|       | 68/313 [12:29<45:01,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  22%|       | 69/313 [12:40<44:49,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  22%|       | 70/313 [12:51<44:38,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  23%|       | 71/313 [13:03<44:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  23%|       | 72/313 [13:12<44:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  23%|       | 73/313 [13:22<43:58,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  24%|       | 74/313 [13:34<43:51,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  24%|       | 75/313 [13:43<43:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  24%|       | 76/313 [13:55<43:25,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  25%|       | 77/313 [14:01<42:59,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  25%|       | 78/313 [14:13<42:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  25%|       | 79/313 [14:25<42:44,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  26%|       | 80/313 [14:39<42:40,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  26%|       | 81/313 [14:51<42:34,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  26%|       | 82/313 [15:02<42:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  27%|       | 83/313 [15:12<42:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  27%|       | 84/313 [15:22<41:55,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  27%|       | 85/313 [15:30<41:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  27%|       | 86/313 [15:43<41:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  28%|       | 87/313 [15:54<41:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  28%|       | 88/313 [16:08<41:17,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  28%|       | 89/313 [16:18<41:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  29%|       | 90/313 [16:32<41:00,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  29%|       | 91/313 [16:47<40:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  29%|       | 92/313 [16:58<40:46,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  30%|       | 93/313 [17:12<40:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  30%|       | 94/313 [17:28<40:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  30%|       | 95/313 [17:37<40:26,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  31%|       | 96/313 [17:43<40:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  31%|       | 97/313 [18:00<40:07,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  31%|      | 98/313 [18:12<39:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  32%|      | 99/313 [18:23<39:45,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  32%|      | 100/313 [18:33<39:30,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  32%|      | 101/313 [18:46<39:25,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  33%|      | 102/313 [19:02<39:24,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  33%|      | 103/313 [19:16<39:17,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  33%|      | 104/313 [19:22<38:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  34%|      | 105/313 [19:32<38:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  34%|      | 106/313 [19:44<38:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  34%|      | 107/313 [19:56<38:22,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  35%|      | 108/313 [20:05<38:08,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  35%|      | 109/313 [20:16<37:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  35%|      | 110/313 [20:30<37:51,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  35%|      | 111/313 [20:40<37:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  36%|      | 112/313 [20:49<37:23,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  36%|      | 113/313 [20:59<37:08,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  36%|      | 114/313 [21:13<37:02,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  37%|      | 115/313 [21:23<36:50,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  37%|      | 116/313 [21:35<36:39,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  37%|      | 117/313 [21:48<36:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  38%|      | 118/313 [21:59<36:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  38%|      | 119/313 [22:12<36:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  38%|      | 120/313 [22:25<36:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  39%|      | 121/313 [22:34<35:49,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  39%|      | 122/313 [22:51<35:46,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  39%|      | 123/313 [23:02<35:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  40%|      | 124/313 [23:15<35:27,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  40%|      | 125/313 [23:24<35:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  40%|      | 126/313 [23:37<35:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  41%|      | 127/313 [23:55<35:02,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  41%|      | 128/313 [24:12<34:58,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  41%|      | 129/313 [24:22<34:45,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  42%|     | 130/313 [24:32<34:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  42%|     | 131/313 [24:45<34:23,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  42%|     | 132/313 [24:54<34:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  42%|     | 133/313 [25:09<34:02,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  43%|     | 134/313 [25:26<33:58,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  43%|     | 135/313 [25:34<33:43,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  43%|     | 136/313 [25:44<33:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  44%|     | 137/313 [26:00<33:25,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  44%|     | 138/313 [26:12<33:14,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  44%|     | 139/313 [26:22<33:01,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  45%|     | 140/313 [26:32<32:48,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  45%|     | 141/313 [26:45<32:37,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  45%|     | 142/313 [26:57<32:28,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  46%|     | 143/313 [27:11<32:19,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  46%|     | 144/313 [27:25<32:10,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  46%|     | 145/313 [27:45<32:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  47%|     | 146/313 [27:55<31:56,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  47%|     | 147/313 [28:10<31:48,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  47%|     | 148/313 [28:18<31:34,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  48%|     | 149/313 [28:29<31:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  48%|     | 150/313 [28:40<31:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  48%|     | 151/313 [28:54<31:01,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  49%|     | 152/313 [29:02<30:45,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  49%|     | 153/313 [29:13<30:33,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  49%|     | 154/313 [29:24<30:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  50%|     | 155/313 [29:32<30:07,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  50%|     | 156/313 [29:42<29:53,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  50%|     | 157/313 [29:59<29:47,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  50%|     | 158/313 [30:11<29:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  51%|     | 159/313 [30:20<29:22,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  51%|     | 160/313 [30:32<29:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  51%|    | 161/313 [30:47<29:04,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  52%|    | 162/313 [30:59<28:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  52%|    | 163/313 [31:09<28:40,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  52%|    | 164/313 [31:24<28:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  53%|    | 165/313 [31:36<28:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  53%|    | 166/313 [31:50<28:11,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  53%|    | 167/313 [31:59<27:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  54%|    | 168/313 [32:13<27:48,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  54%|    | 169/313 [32:23<27:35,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  54%|    | 170/313 [32:35<27:25,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  55%|    | 171/313 [32:48<27:14,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  55%|    | 172/313 [33:00<27:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  55%|    | 173/313 [33:14<26:54,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  56%|    | 174/313 [33:29<26:44,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  56%|    | 175/313 [33:38<26:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  56%|    | 176/313 [33:47<26:18,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  57%|    | 177/313 [33:58<26:06,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  57%|    | 178/313 [34:06<25:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  57%|    | 179/313 [34:21<25:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  58%|    | 180/313 [34:34<25:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  58%|    | 181/313 [34:44<25:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  58%|    | 182/313 [34:57<25:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  58%|    | 183/313 [35:08<24:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  59%|    | 184/313 [35:23<24:48,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  59%|    | 185/313 [35:31<24:35,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  62%|   | 194/313 [37:19<22:53,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  62%|   | 195/313 [37:30<22:41,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  63%|   | 196/313 [37:43<22:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  63%|   | 197/313 [37:52<22:18,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  63%|   | 198/313 [38:00<22:04,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  64%|   | 199/313 [38:15<21:54,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  64%|   | 200/313 [38:27<21:43,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  64%|   | 201/313 [38:37<21:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  65%|   | 202/313 [38:49<21:20,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  65%|   | 203/313 [38:57<21:06,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  65%|   | 204/313 [39:07<20:54,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  65%|   | 205/313 [39:19<20:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  66%|   | 206/313 [39:33<20:33,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  66%|   | 207/313 [39:45<20:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  66%|   | 208/313 [40:01<20:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  67%|   | 209/313 [40:09<19:58,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  67%|   | 210/313 [40:22<19:48,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  67%|   | 211/313 [40:32<19:36,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  68%|   | 212/313 [40:46<19:25,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  68%|   | 213/313 [40:59<19:14,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  68%|   | 214/313 [41:08<19:01,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  69%|   | 215/313 [41:18<18:49,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  69%|   | 216/313 [41:32<18:39,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  69%|   | 217/313 [41:44<18:27,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  70%|   | 218/313 [41:56<18:16,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  70%|   | 219/313 [42:07<18:04,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  70%|   | 220/313 [42:20<17:53,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  71%|   | 221/313 [42:28<17:40,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  71%|   | 222/313 [42:39<17:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  71%|   | 223/313 [42:48<17:16,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  72%|  | 224/313 [43:00<17:05,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  72%|  | 225/313 [43:13<16:54,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  72%|  | 226/313 [43:25<16:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  73%|  | 227/313 [43:38<16:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  73%|  | 228/313 [43:51<16:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  73%|  | 229/313 [44:02<16:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  73%|  | 230/313 [44:12<15:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  74%|  | 231/313 [44:21<15:44,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  74%|  | 232/313 [44:38<15:35,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  74%|  | 233/313 [44:51<15:24,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  75%|  | 234/313 [45:02<15:12,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  75%|  | 235/313 [45:16<15:01,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  75%|  | 236/313 [45:30<14:50,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  76%|  | 237/313 [45:39<14:38,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  76%|  | 238/313 [45:49<14:26,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  76%|  | 239/313 [46:03<14:15,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  77%|  | 240/313 [46:16<14:04,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  77%|  | 241/313 [46:25<13:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  77%|  | 242/313 [46:38<13:40,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  78%|  | 243/313 [46:49<13:29,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  78%|  | 244/313 [47:00<13:17,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  78%|  | 245/313 [47:10<13:05,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  79%|  | 246/313 [47:22<12:54,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  79%|  | 247/313 [47:32<12:42,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  79%|  | 248/313 [47:43<12:30,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  80%|  | 249/313 [47:53<12:18,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  80%|  | 250/313 [48:06<12:07,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  80%|  | 251/313 [48:17<11:55,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  81%|  | 252/313 [48:26<11:43,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  81%|  | 253/313 [48:35<11:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  81%|  | 254/313 [48:54<11:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  81%| | 255/313 [49:06<11:10,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  82%| | 256/313 [49:16<10:58,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  82%| | 257/313 [49:28<10:46,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  82%| | 258/313 [49:41<10:35,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  83%| | 259/313 [49:53<10:24,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  83%| | 260/313 [50:11<10:13,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  83%| | 261/313 [50:25<10:02,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  84%| | 262/313 [50:40<09:51,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  84%| | 263/313 [50:50<09:39,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  84%| | 264/313 [50:59<09:27,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  85%| | 265/313 [51:07<09:15,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  85%| | 266/313 [51:15<09:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  85%| | 267/313 [51:28<08:52,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  86%| | 268/313 [51:35<08:39,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  86%| | 269/313 [51:49<08:28,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  86%| | 270/313 [52:05<08:17,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  87%| | 271/313 [52:16<08:06,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  87%| | 272/313 [52:26<07:54,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  87%| | 273/313 [52:40<07:43,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  88%| | 274/313 [52:50<07:31,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  88%| | 275/313 [53:03<07:19,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  88%| | 276/313 [53:12<07:08,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  88%| | 277/313 [53:24<06:56,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  89%| | 278/313 [53:36<06:44,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  89%| | 279/313 [53:47<06:33,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  89%| | 280/313 [53:59<06:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  90%| | 281/313 [54:12<06:10,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  90%| | 282/313 [54:22<05:58,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  90%| | 283/313 [54:34<05:47,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  91%| | 284/313 [54:44<05:35,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  91%| | 285/313 [54:55<05:23,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  91%|| 286/313 [55:04<05:11,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  92%|| 287/313 [55:15<05:00,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  92%|| 288/313 [55:29<04:49,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  92%|| 289/313 [55:42<04:37,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  93%|| 290/313 [55:58<04:26,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  93%|| 291/313 [56:08<04:14,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  93%|| 292/313 [56:22<04:03,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  96%|| 301/313 [58:05<02:18,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  96%|| 302/313 [58:17<02:07,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  97%|| 303/313 [58:32<01:55,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  97%|| 304/313 [58:46<01:44,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  97%|| 305/313 [58:53<01:32,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  98%|| 306/313 [59:02<01:21,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  98%|| 307/313 [59:16<01:09,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  98%|| 308/313 [59:29<00:57,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  99%|| 309/313 [59:41<00:46,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  99%|| 310/313 [59:50<00:34,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0:  99%|| 311/313 [1:00:00<00:23,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0: 100%|| 312/313 [1:00:11<00:11,  0.09it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 16\n",
      "Testing DataLoader 0: 100%|| 313/313 [1:00:25<00:00,  0.09it/s][NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:553] skipping autogenerated example example Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\n",
      "    \n",
      "    TITLE:\n",
      "    Can a Statutory Framework Limit the Power of a Future Parliament?\n",
      "    \n",
      "    QUESTION:\n",
      "    Is it constitutionally permissible for a current Parliament to enact legislation that imposes procedural restrictions on the passage of future bills, potentially fettering the ability of a future Parliament to amend or repeal existing laws? Or would such a statutory framework be considered an impermissible attempt to bind the hands of future lawmakers, contrary to the principles of parliamentary sovereignty and the rule of law? How might the courts address such a challenge, and what implications might this have for the balance of power between the legislative and judicial branches of government? prediction  constitutional-law,parliament,parliamentary-sovereignty label  \n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:586] Total deduplicated inference data size: 5008 to 5000\n",
      "[NeMo I 2024-10-28 08:54:08 megatron_gpt_sft_model:737] Predictions saved to results/inference/infer-submission.jsonl_test_infer_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 08:54:08 megatron_gpt_sft_model:677] No training data found, reconfiguring microbatches based on validation batch sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-10-28 08:54:08 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 08:54:08 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss_infer', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-10-28 08:54:08 nemo_logging:361] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 313/313 [1:00:25<00:00,  0.09it/s]\n",
      "\n",
      "\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m\n",
      "\n",
      "\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    10.21175479888916    \u001b[0m\u001b[35m \u001b[0m\n",
      "\u001b[36m \u001b[0m\u001b[36m     test_loss_infer     \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    10.21175479888916    \u001b[0m\u001b[35m \u001b[0m\n",
      "\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m\u001b[35m \u001b[0m\u001b[35m    10.21175479888916    \u001b[0m\u001b[35m \u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# This is where the inference results will be stored.\n",
    "OUTPUT_DIR=\"results/inference/infer-$TEST_FP\"\n",
    "\n",
    "SCHEME=\"lora\"\n",
    "TP_SIZE=1\n",
    "PP_SIZE=1\n",
    "\n",
    "# Clear up cached mem-map file\n",
    "rm $DATA_DIR/*idx*\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "    model.restore_from_path=${BASE_MODEL} \\\n",
    "    model.peft.restore_from_path=${TRAINED_ADAPTER} \\\n",
    "    trainer.devices=1 \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    inference.greedy=True \\\n",
    "    model.data.test_ds.file_names=[${TEST_DS}] \\\n",
    "    model.data.test_ds.names=[\"infer\"] \\\n",
    "    model.data.test_ds.global_batch_size=16 \\\n",
    "    model.data.test_ds.micro_batch_size=1 \\\n",
    "    model.data.test_ds.tokens_to_generate=32 \\\n",
    "    model.tensor_model_parallel_size=${TP_SIZE} \\\n",
    "    model.pipeline_model_parallel_size=${PP_SIZE} \\\n",
    "    model.data.test_ds.output_file_path_prefix=$OUTPUT_DIR \\\n",
    "    model.data.test_ds.write_predictions_to_file=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d5e7ee",
   "metadata": {},
   "source": [
    "The results will be written under `results/inference`. Please send us this file for your final submission.\n",
    "\n",
    "Let's inspect a couple of lines from that file for sanity checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b4726de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\\n\\nTITLE:\\nFairness in Punishment for Reckless Behavior\\n\\nQUESTION:\\nIs it justifiable to have significantly different penalties for individuals who engage in reckless behavior, depending on the outcome of their actions, or should the focus be on the level of recklessness itself, regardless of the consequences?\", \"pred\": \" criminal-law,punishment\", \"label\": \" \", \"filename\": \"submission.jsonl\"}\n",
      "{\"input\": \"Read the following title and question about a legal issue and assign the most appropriate tag to it. All tags must be in lowercase, ordered lexicographically and separated by commas.\\n\\nTITLE:\\nCan a Promise Be Binding Without a Tangible Exchange?\\n\\nQUESTION:\\nIn what situations can a commitment or promise be deemed legally enforceable, even if no direct benefit or tangible item is exchanged between parties, and what conditions must be met for such a promise to hold weight in a court of law?\", \"pred\": \" contract-law\", \"label\": \" \", \"filename\": \"submission.jsonl\"}\n",
      "cat: write error: Broken pipe\n"
     ]
    }
   ],
   "source": [
    "! cat results/inference/infer-submission.jsonl_test_infer_inputs_preds_labels.jsonl | head -n 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58554da2",
   "metadata": {},
   "source": [
    "---\n",
    "# Freeing Memory and Other Resources\n",
    "\n",
    "As always, it is a good idea to free up all allocated resources when you are done. Please execute the following cell to do so.\n",
    "\n",
    "Alternatively, please restart the kernel by navigating to `Kernel > Restart Kernel` (if using Jypyter notebook), or clicking the `Restart` button in VS Code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf196ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exit(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
